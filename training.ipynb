{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Test Accuracy: 0.958 %\n",
      "F1 Score: 0.961\n",
      "ROC AUC Score: 0.957\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Test Accuracy: 0.966 %\n",
      "F1 Score: 0.968\n",
      "ROC AUC Score: 0.965\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Test Accuracy: 0.964 %\n",
      "F1 Score: 0.965\n",
      "ROC AUC Score: 0.966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Test Accuracy: 0.935 %\n",
      "F1 Score: 0.939\n",
      "ROC AUC Score: 0.934\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Test Accuracy: 0.982 %\n",
      "F1 Score: 0.983\n",
      "ROC AUC Score: 0.981\n",
      "\n",
      "Model: Decision Tree\n",
      "Test Accuracy: 0.94 %\n",
      "F1 Score: 0.944\n",
      "ROC AUC Score: 0.939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/ammar-asim/misc/shine/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ada Boost\n",
      "Test Accuracy: 0.948 %\n",
      "F1 Score: 0.953\n",
      "ROC AUC Score: 0.947\n",
      "\n",
      "Model: XG Boost\n",
      "Test Accuracy: 0.962 %\n",
      "F1 Score: 0.965\n",
      "ROC AUC Score: 0.961\n",
      "\n",
      "Model: Naive Bayes\n",
      "Test Accuracy: 0.906 %\n",
      "F1 Score: 0.912\n",
      "ROC AUC Score: 0.906\n",
      "\n",
      "Best Model:\n",
      "Test Accuracy: 0.9819432502149613\n",
      "F1 Score: 0.9832935560859188\n",
      "ROC AUC Score: 0.9808917008257589\n",
      "Model Pipeline: KNeighborsClassifier(n_neighbors=3, weights='distance') with accuracy 0.98 %\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "feature_imp_dict = {}\n",
    "\n",
    "df = pd.read_csv('cleaned_dataset.csv',index_col=0)\n",
    "X = df.drop(['Churn'],axis=1)\n",
    "y = df['Churn']\n",
    "# UpSampling\n",
    "sm = SMOTEENN()\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_res, y_res, test_size=0.2)\n",
    "\n",
    "# Initialize an empty list to store model scores\n",
    "model_scores = []\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42),\n",
    "        {'n_estimators': [50, 100, 200],\n",
    "         'max_depth': [None, 10, 20]}),  # Add hyperparameters for Random Forest\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42),\n",
    "        {'n_estimators': [50, 100, 200],\n",
    "         'learning_rate': [0.05, 0.1, 0.5]}),  # Add hyperparameters for Gradient Boosting\n",
    "    ('Support Vector Machine', SVC(random_state=42, class_weight='balanced'),\n",
    "        {'C': [0.1, 1, 10],\n",
    "         'gamma': ['scale', 'auto']}),  # Add hyperparameters for SVM\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "        {'C': [0.1, 1, 10],\n",
    "         'penalty': ['l2']}),  # Add hyperparameters for Logistic Regression\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(),\n",
    "        {'n_neighbors': [3, 5, 7],\n",
    "         'weights': ['uniform', 'distance']}),  # Add hyperparameters for KNN\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42),\n",
    "        {'max_depth': [None, 10, 20],\n",
    "         'min_samples_split': [2, 5, 10]}),  # Add hyperparameters for Decision Tree\n",
    "    ('Ada Boost', AdaBoostClassifier(random_state=42),\n",
    "        {'n_estimators': [50, 100, 200],\n",
    "         'learning_rate': [0.05, 0.1, 0.5]}),  # Add hyperparameters for Ada Boost\n",
    "    ('XG Boost', XGBClassifier(random_state=42),\n",
    "        {'max_depth': randint(3, 6), \n",
    "         'learning_rate': uniform(0.01, 0.2),  \n",
    "         'n_estimators': randint(100, 300),  \n",
    "         'subsample': uniform(0.8, 0.2)}),  # Add hyperparameters for XG Boost\n",
    "    ('Naive Bayes', GaussianNB(), {})  # No hyperparameters for Naive Bayes\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "best_f1 = 0.0\n",
    "best_auc_roc = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance\n",
    "for name, model, param_grid in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),  # Feature Scaling\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter tuning using RandomizedSearchCV for XG Boost\n",
    "    if name == 'XG Boost':\n",
    "        random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                                           n_iter=100, cv=3, verbose=0, random_state=42, n_jobs=-1)\n",
    "        random_search.fit(Xr_train, yr_train)\n",
    "        pipeline = random_search.best_estimator_\n",
    "    # Hyperparameter tuning using GridSearchCV for other models\n",
    "    elif param_grid:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=2, verbose=0)\n",
    "        grid_search.fit(Xr_train, yr_train)\n",
    "        pipeline = grid_search.best_estimator_\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(Xr_train, yr_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(Xr_test)\n",
    "\n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(yr_test, y_pred)\n",
    "\n",
    "    #Calculate f1 score\n",
    "    f1 = f1_score(yr_test, y_pred)\n",
    "\n",
    "    # Calculate AUC-ROC score\n",
    "    auc_roc = roc_auc_score(yr_test, y_pred) \n",
    "\n",
    "    if  name in ['Random Forest', 'Gradient Boosting', 'Ada Boost', 'XG Boost', 'Decision Tree']:\n",
    "        importances = pipeline.feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importances = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "        feature_imp_dict[name] = feature_importances\n",
    "\n",
    "    elif name == 'Logistic Regression':\n",
    "        coef = pipeline.coef_[0]\n",
    "        feature_importances = pd.DataFrame(coef, index=X.columns, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "        feature_imp_dict[name] = feature_importances\n",
    "\n",
    "    elif name in ['K-Nearest Neighbors','Naive Bayes', 'Support Vector Machine']:    \n",
    "        result = permutation_importance(pipeline, Xr_test, yr_test, n_repeats=10, random_state=42)\n",
    "        feature_importances = pd.DataFrame(result.importances_mean, index=X.columns, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "        feature_imp_dict[name] = feature_importances\n",
    "\n",
    "    # Append model name and accuracy to the list\n",
    "    model_scores.append({'Model': name, 'Accuracy': accuracy})\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    scores_df = pd.DataFrame(model_scores)\n",
    "\n",
    "    # Print the performance metrics\n",
    "    print(\"Model:\", name)\n",
    "    print(\"Test Accuracy:\", round(accuracy, 3),\"%\")\n",
    "    print(\"F1 Score:\", round(f1,3))\n",
    "    print(\"ROC AUC Score:\", round(auc_roc,3))\n",
    "    print()\n",
    "\n",
    "    # Save the models to a pickle file\n",
    "    with open(f'models/{name}.pkl','wb') as fp:\n",
    "        pickle.dump(pipeline, fp)\n",
    "\n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_f1 = f1\n",
    "        best_auc_roc = auc_roc\n",
    "        best_model = pipeline\n",
    "\n",
    "# Retrieve the overall best model\n",
    "print(\"Best Model:\")\n",
    "print(\"Test Accuracy:\", best_accuracy)\n",
    "print(\"F1 Score:\", best_f1)\n",
    "print(\"ROC AUC Score:\", best_auc_roc)\n",
    "print(\"Model Pipeline:\", best_model, \"with accuracy\", round(best_accuracy, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance for Random Forest:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "Contract            0.199789\n",
      "tenure              0.180900\n",
      "MonthlyCharges      0.101548\n",
      "OnlineSecurity      0.100847\n",
      "TotalCharges        0.100756\n",
      "TechSupport         0.085109\n",
      "InternetService     0.057598\n",
      "OnlineBackup        0.039588\n",
      "DeviceProtection    0.028089\n",
      "PaymentMethod       0.018662\n",
      "Dependents          0.015825\n",
      "Partner             0.015394\n",
      "StreamingTV         0.010856\n",
      "StreamingMovies     0.010071\n",
      "gender              0.009854\n",
      "MultipleLines       0.008600\n",
      "PaperlessBilling    0.007461\n",
      "SeniorCitizen       0.005519\n",
      "PhoneService        0.003535\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for Gradient Boosting:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "Contract            0.580647\n",
      "tenure              0.121202\n",
      "MonthlyCharges      0.090292\n",
      "TotalCharges        0.085427\n",
      "InternetService     0.067164\n",
      "TechSupport         0.013331\n",
      "OnlineSecurity      0.009196\n",
      "OnlineBackup        0.007528\n",
      "PaymentMethod       0.005179\n",
      "Dependents          0.004340\n",
      "MultipleLines       0.002619\n",
      "SeniorCitizen       0.002366\n",
      "DeviceProtection    0.002172\n",
      "PaperlessBilling    0.002039\n",
      "StreamingTV         0.002028\n",
      "gender              0.001719\n",
      "Partner             0.001535\n",
      "PhoneService        0.000719\n",
      "StreamingMovies     0.000497\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for Support Vector Machine:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "TotalCharges        0.447549\n",
      "tenure              0.377472\n",
      "MonthlyCharges      0.371367\n",
      "Contract            0.002236\n",
      "StreamingTV         0.001376\n",
      "PaymentMethod       0.001204\n",
      "StreamingMovies     0.000688\n",
      "OnlineSecurity      0.000688\n",
      "PaperlessBilling    0.000602\n",
      "MultipleLines       0.000516\n",
      "DeviceProtection    0.000516\n",
      "gender              0.000430\n",
      "OnlineBackup        0.000430\n",
      "Dependents          0.000172\n",
      "SeniorCitizen       0.000000\n",
      "Partner             0.000000\n",
      "PhoneService        0.000000\n",
      "InternetService     0.000000\n",
      "TechSupport        -0.000258\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for Logistic Regression:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "InternetService     0.456468\n",
      "PaperlessBilling    0.259925\n",
      "PaymentMethod       0.069234\n",
      "MonthlyCharges      0.061924\n",
      "TotalCharges        0.001223\n",
      "StreamingMovies    -0.058378\n",
      "StreamingTV        -0.085999\n",
      "tenure             -0.187122\n",
      "MultipleLines      -0.216107\n",
      "SeniorCitizen      -0.238588\n",
      "PhoneService       -0.280188\n",
      "DeviceProtection   -0.360974\n",
      "Partner            -0.419990\n",
      "gender             -0.443873\n",
      "OnlineBackup       -0.510363\n",
      "OnlineSecurity     -0.511429\n",
      "Dependents         -0.680224\n",
      "TechSupport        -0.763374\n",
      "Contract           -1.205329\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for K-Nearest Neighbors:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "MonthlyCharges      0.296733\n",
      "TotalCharges        0.279966\n",
      "tenure              0.071281\n",
      "PaymentMethod       0.001720\n",
      "Contract            0.000946\n",
      "PaperlessBilling    0.000086\n",
      "Partner             0.000000\n",
      "DeviceProtection    0.000000\n",
      "OnlineBackup        0.000000\n",
      "PhoneService        0.000000\n",
      "Dependents          0.000000\n",
      "MultipleLines       0.000000\n",
      "StreamingMovies     0.000000\n",
      "OnlineSecurity      0.000000\n",
      "InternetService     0.000000\n",
      "TechSupport         0.000000\n",
      "StreamingTV         0.000000\n",
      "gender             -0.000430\n",
      "SeniorCitizen      -0.000688\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for Decision Tree:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "Contract            0.534657\n",
      "TotalCharges        0.119488\n",
      "MonthlyCharges      0.107362\n",
      "tenure              0.080831\n",
      "InternetService     0.070151\n",
      "PaymentMethod       0.014998\n",
      "TechSupport         0.010704\n",
      "OnlineSecurity      0.009869\n",
      "gender              0.009167\n",
      "Dependents          0.007443\n",
      "SeniorCitizen       0.005915\n",
      "OnlineBackup        0.005581\n",
      "MultipleLines       0.005290\n",
      "PhoneService        0.004329\n",
      "PaperlessBilling    0.003639\n",
      "DeviceProtection    0.003430\n",
      "StreamingTV         0.002898\n",
      "Partner             0.002408\n",
      "StreamingMovies     0.001840\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for Ada Boost:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "TotalCharges           0.255\n",
      "MonthlyCharges         0.245\n",
      "tenure                 0.175\n",
      "TechSupport            0.055\n",
      "OnlineSecurity         0.055\n",
      "OnlineBackup           0.025\n",
      "DeviceProtection       0.025\n",
      "PhoneService           0.025\n",
      "Contract               0.020\n",
      "InternetService        0.020\n",
      "StreamingMovies        0.020\n",
      "StreamingTV            0.020\n",
      "MultipleLines          0.015\n",
      "gender                 0.015\n",
      "SeniorCitizen          0.010\n",
      "Dependents             0.010\n",
      "Partner                0.005\n",
      "PaymentMethod          0.005\n",
      "PaperlessBilling       0.000\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for XG Boost:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "Contract            0.654443\n",
      "InternetService     0.130281\n",
      "TechSupport         0.023324\n",
      "Dependents          0.023067\n",
      "tenure              0.022033\n",
      "OnlineSecurity      0.017877\n",
      "MonthlyCharges      0.016280\n",
      "OnlineBackup        0.016168\n",
      "DeviceProtection    0.011127\n",
      "TotalCharges        0.010829\n",
      "PhoneService        0.009766\n",
      "SeniorCitizen       0.009382\n",
      "StreamingMovies     0.008740\n",
      "gender              0.008523\n",
      "MultipleLines       0.008490\n",
      "Partner             0.008328\n",
      "StreamingTV         0.007590\n",
      "PaperlessBilling    0.007140\n",
      "PaymentMethod       0.006612\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Feature Importance for Naive Bayes:\n",
      "--------------------------------\n",
      "                  Importance\n",
      "Contract            0.126913\n",
      "MonthlyCharges      0.019518\n",
      "OnlineSecurity      0.018143\n",
      "InternetService     0.016079\n",
      "tenure              0.015821\n",
      "TechSupport         0.013500\n",
      "Dependents          0.012726\n",
      "OnlineBackup        0.008942\n",
      "PaperlessBilling    0.005159\n",
      "gender              0.004385\n",
      "TotalCharges        0.004127\n",
      "SeniorCitizen       0.003869\n",
      "PhoneService        0.003439\n",
      "StreamingMovies     0.003353\n",
      "Partner             0.003267\n",
      "PaymentMethod       0.002236\n",
      "DeviceProtection    0.001548\n",
      "StreamingTV        -0.000086\n",
      "MultipleLines      -0.000688\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in feature_imp_dict.items():\n",
    "    print(f\"\\nFeature Importance for {i}:\")\n",
    "    print('--------------------------------')\n",
    "    print(j)\n",
    "    print('--------------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
